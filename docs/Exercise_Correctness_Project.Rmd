---
title: "Weight Lifting Correctness Prediction Project"
author: "Josiah Green"
date: "October 31, 2020"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

## Summary
The goal of this project is to predict the manner in which a group of people conducted weight lifting exercises (correctly or incorrectly) based on biometric data. 

Based on the analysis, the Random Forest model was selected due to its low out-of-sample error (0.006). This model was then used to predict the outcome of 20 workouts. 

This report will contain the following sections: \
1. Exploratory Analysis - First the data will be cleaned and analyzed. \
2. Model Building & Analysis - The description of the model is defined, as well as the cross validation methodology and expected sample error. \
3. Prediction Cases - The prediction model is used to predict 20 different test cases.\

##Exploratory Analysis
First we load in the data and clean it. We only look at the training data. We remove missing or Null data, and we classify the response as "correct" or "incorrect". Class A corresponds to correct lift, while classes B through E correspond to common mistakes.

```{r}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv") #training data
training <- read.csv("pml-training.csv", header=T, sep=",",na.strings=c("", "NA"),stringsAsFactors=F)

download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv") #test data
testing <- read.csv("pml-testing.csv", header=T, sep=",",na.strings=c("","NA"),stringsAsFactors=F) 


trainingClean <- training[which(colSums(is.na(training))==0)]
trainingClean <- trainingClean[-which(names(trainingClean) %in% c("X","user_name", "cvtd_timestamp","raw_timestamp_part_1", "raw_timestamp_part_2","new_window", "num_window"))]

testingClean <- testing[which(colSums(is.na(testing))==0)]
testingClean <- testingClean[-which(names(testingClean) %in% c("X","user_name", "cvtd_timestamp","raw_timestamp_part_1", "raw_timestamp_part_2","new_window", "num_window"))]
```

Now that we have clean data, let's visualize our feature space to understand the relationships that exist in the data. For the purpose of this excercise, we will plot the linear corrlelation between features. As we see in the below plot, we have a good amount of relatively uncorrelated predictors for the model to utilize.

```{r}

trainingCleanFeatures <- trainingClean[-which(names(trainingClean) %in% c("classe"))]
corMatrix <- cor(trainingCleanFeatures)

library(corrplot)
corrplot(corMatrix, tl.col="black")

```

##Model Building & Analysis
For the purposes of predicting a classificatoin, let's start with a simple decision tree model. This will help in the event that the relatoinships between features and predictor is non-linear. In order to properly train the model, cross validation is utilized. For computation efficiency, parallelization is enabled as well (this is especially important for the random forest model).

```{r}
set.seed(444)

library(caret)
inTrain <- createDataPartition(trainingClean$classe, p=0.7, list=FALSE)
trainData <- trainingClean[inTrain,]
testData <- trainingClean[-inTrain,]

trainParam <- trainControl(method='cv',number = 3, allowParallel=TRUE)

tree <- train(classe~., data=trainData, method="rpart",trControl=trainParam)
library(rattle)
fancyRpartPlot(tree$finalModel)

```

It looks like the model is predicting about 52% "correct" lifts and omits lift type "D" entirely. Based on our exploratory analysis, we know that this isn't very accurate to reality. However, we can test it to see how it performs. Let's validate the model with the testing data to see what (if any) changes we need to make.

```{r}

tree_predict <- predict(tree, newdata=testData)

testData$classe <- as.factor(testData$classe)
tree_perfSummary <- confusionMatrix(testData$classe, tree_predict)

tree_perfSummary

```

We see that the accuracy of the simple tree model is not very good at just 0.4975. This means the out-of-sample error is greater than 50% which is worse than flipping a coin. We need a better model for classification.

Next we will try a Random Forest model. Random Forest gets the benefits of tree-like handling of all kinds of data (no linearity assumptions required) while leveraging the power of models-of-models. We should expect much better performance here.

```{r}
library(parallel)
library(doParallel)

cluster <- makeCluster(detectCores()-1) #turn on cluster
registerDoParallel(cluster) #register the cluster

rf <- train(classe~., data=trainData, method="rf", trainControl = trainParam, verbose = FALSE)

stopCluster(cluster) #shut down cluster
registerDoSEQ() #un-register cluster

rf_predict <- predict(rf, newdata=testData)
rf_perfSummary <- confusionMatrix(testData$classe, rf_predict)

rf_perfSummary

```

We can see that the accuracy is much better for the random forest model. Its accuracy is very high at 0.994, which means the out-of-sample error is 0.006.

##Predicted Cases

Now, let's use our selected model to predict the outcome of 20 workouts. 

```{r}

workout_prediction <- predict(rf, newdata=testingClean)
workout_prediction

```


##References
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. \

Read more: http://groupware.les.inf.puc-rio.br/har#dataset#ixzz69iER7ja0

